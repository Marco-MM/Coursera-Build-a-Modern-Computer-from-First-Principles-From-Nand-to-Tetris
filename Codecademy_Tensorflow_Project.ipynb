{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marco-MM/Coursera-Build-a-Modern-Computer-from-First-Principles-From-Nand-to-Tetris/blob/master/Codecademy_Tensorflow_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0No5Ww1a6_b"
      },
      "source": [
        "# **Notebook for Final Project of Codecademy's Build Deep Learning Models with TensorFlow**\n",
        "\n",
        "Project: Forest Cover Classification\n",
        "\n",
        "[Link to Codecademy Page](https://www.codecademy.com/paths/build-deep-learning-models-with-tensorflow/tracks/dlsp-deep-learning-portfolio-project-track/modules/dlsp-deep-learning-portfolio-project-module/kanban_projects/deep-learning-cover-classification-portfolio-project)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ylZwl1a3z7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5B9OUz3cXh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2215df29-17ab-4942-9449-ff69daaf27df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcQ56vf6dvj4"
      },
      "source": [
        "# Prepocess and Explore Dataset\n",
        "\n",
        "[ x ] Upload dataset to repository\n",
        "\n",
        "[ x ] Get stats on dataset (cols, num samples, sample of data)\n",
        "\n",
        "[ x ] Split dataset into 3: training, validation, and test set\n",
        "\n",
        "[ x ] Review data and determine how to preprocess. Write further steps after this one.\n",
        "\n",
        "[ x ] Scale data using columnTransformer and standardScaler\n",
        "\n",
        "[ x ] Convert labels into one-hot-encodings (format needed for our cross-entropy loss)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBZKVf6sdCFp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "9b4edbd7-9316-4d73-9752-88af8546fab2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-194bb8d6-f88b-4be7-9a02-7c2af01f300d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Horizontal_Distance_To_Hydrology</th>\n",
              "      <th>Vertical_Distance_To_Hydrology</th>\n",
              "      <th>Horizontal_Distance_To_Roadways</th>\n",
              "      <th>Hillshade_9am</th>\n",
              "      <th>Hillshade_Noon</th>\n",
              "      <th>Hillshade_3pm</th>\n",
              "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
              "      <th>Wilderness_Area1</th>\n",
              "      <th>Wilderness_Area2</th>\n",
              "      <th>Wilderness_Area3</th>\n",
              "      <th>Wilderness_Area4</th>\n",
              "      <th>Soil_Type1</th>\n",
              "      <th>Soil_Type2</th>\n",
              "      <th>Soil_Type3</th>\n",
              "      <th>Soil_Type4</th>\n",
              "      <th>Soil_Type5</th>\n",
              "      <th>Soil_Type6</th>\n",
              "      <th>Soil_Type7</th>\n",
              "      <th>Soil_Type8</th>\n",
              "      <th>Soil_Type9</th>\n",
              "      <th>Soil_Type10</th>\n",
              "      <th>Soil_Type11</th>\n",
              "      <th>Soil_Type12</th>\n",
              "      <th>Soil_Type13</th>\n",
              "      <th>Soil_Type14</th>\n",
              "      <th>Soil_Type15</th>\n",
              "      <th>Soil_Type16</th>\n",
              "      <th>Soil_Type17</th>\n",
              "      <th>Soil_Type18</th>\n",
              "      <th>Soil_Type19</th>\n",
              "      <th>Soil_Type20</th>\n",
              "      <th>Soil_Type21</th>\n",
              "      <th>Soil_Type22</th>\n",
              "      <th>Soil_Type23</th>\n",
              "      <th>Soil_Type24</th>\n",
              "      <th>Soil_Type25</th>\n",
              "      <th>Soil_Type26</th>\n",
              "      <th>Soil_Type27</th>\n",
              "      <th>Soil_Type28</th>\n",
              "      <th>Soil_Type29</th>\n",
              "      <th>Soil_Type30</th>\n",
              "      <th>Soil_Type31</th>\n",
              "      <th>Soil_Type32</th>\n",
              "      <th>Soil_Type33</th>\n",
              "      <th>Soil_Type34</th>\n",
              "      <th>Soil_Type35</th>\n",
              "      <th>Soil_Type36</th>\n",
              "      <th>Soil_Type37</th>\n",
              "      <th>Soil_Type38</th>\n",
              "      <th>Soil_Type39</th>\n",
              "      <th>Soil_Type40</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596</td>\n",
              "      <td>51</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "      <td>221</td>\n",
              "      <td>232</td>\n",
              "      <td>148</td>\n",
              "      <td>6279</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2590</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>212</td>\n",
              "      <td>-6</td>\n",
              "      <td>390</td>\n",
              "      <td>220</td>\n",
              "      <td>235</td>\n",
              "      <td>151</td>\n",
              "      <td>6225</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804</td>\n",
              "      <td>139</td>\n",
              "      <td>9</td>\n",
              "      <td>268</td>\n",
              "      <td>65</td>\n",
              "      <td>3180</td>\n",
              "      <td>234</td>\n",
              "      <td>238</td>\n",
              "      <td>135</td>\n",
              "      <td>6121</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785</td>\n",
              "      <td>155</td>\n",
              "      <td>18</td>\n",
              "      <td>242</td>\n",
              "      <td>118</td>\n",
              "      <td>3090</td>\n",
              "      <td>238</td>\n",
              "      <td>238</td>\n",
              "      <td>122</td>\n",
              "      <td>6211</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>391</td>\n",
              "      <td>220</td>\n",
              "      <td>234</td>\n",
              "      <td>150</td>\n",
              "      <td>6172</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581007</th>\n",
              "      <td>2396</td>\n",
              "      <td>153</td>\n",
              "      <td>20</td>\n",
              "      <td>85</td>\n",
              "      <td>17</td>\n",
              "      <td>108</td>\n",
              "      <td>240</td>\n",
              "      <td>237</td>\n",
              "      <td>118</td>\n",
              "      <td>837</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581008</th>\n",
              "      <td>2391</td>\n",
              "      <td>152</td>\n",
              "      <td>19</td>\n",
              "      <td>67</td>\n",
              "      <td>12</td>\n",
              "      <td>95</td>\n",
              "      <td>240</td>\n",
              "      <td>237</td>\n",
              "      <td>119</td>\n",
              "      <td>845</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581009</th>\n",
              "      <td>2386</td>\n",
              "      <td>159</td>\n",
              "      <td>17</td>\n",
              "      <td>60</td>\n",
              "      <td>7</td>\n",
              "      <td>90</td>\n",
              "      <td>236</td>\n",
              "      <td>241</td>\n",
              "      <td>130</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581010</th>\n",
              "      <td>2384</td>\n",
              "      <td>170</td>\n",
              "      <td>15</td>\n",
              "      <td>60</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>230</td>\n",
              "      <td>245</td>\n",
              "      <td>143</td>\n",
              "      <td>864</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581011</th>\n",
              "      <td>2383</td>\n",
              "      <td>165</td>\n",
              "      <td>13</td>\n",
              "      <td>60</td>\n",
              "      <td>4</td>\n",
              "      <td>67</td>\n",
              "      <td>231</td>\n",
              "      <td>244</td>\n",
              "      <td>141</td>\n",
              "      <td>875</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>581012 rows Ã— 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-194bb8d6-f88b-4be7-9a02-7c2af01f300d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-194bb8d6-f88b-4be7-9a02-7c2af01f300d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-194bb8d6-f88b-4be7-9a02-7c2af01f300d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Elevation  Aspect  Slope  ...  Soil_Type39  Soil_Type40  class\n",
              "0            2596      51      3  ...            0            0      5\n",
              "1            2590      56      2  ...            0            0      5\n",
              "2            2804     139      9  ...            0            0      2\n",
              "3            2785     155     18  ...            0            0      2\n",
              "4            2595      45      2  ...            0            0      5\n",
              "...           ...     ...    ...  ...          ...          ...    ...\n",
              "581007       2396     153     20  ...            0            0      3\n",
              "581008       2391     152     19  ...            0            0      3\n",
              "581009       2386     159     17  ...            0            0      3\n",
              "581010       2384     170     15  ...            0            0      3\n",
              "581011       2383     165     13  ...            0            0      3\n",
              "\n",
              "[581012 rows x 55 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "cover_dataframe = pd.read_csv('/content/drive/MyDrive/Codecademy Projects/Tensorflow/cover_data.csv')\n",
        "\n",
        "# Visualize dataset \n",
        "cover_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdvlH100dj2C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "cc0165c0-7bc9-480e-f5a6-03f419cd432a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5ff4d8bb-03c2-4fd2-b8c1-b807c4a535b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Horizontal_Distance_To_Hydrology</th>\n",
              "      <th>Vertical_Distance_To_Hydrology</th>\n",
              "      <th>Horizontal_Distance_To_Roadways</th>\n",
              "      <th>Hillshade_9am</th>\n",
              "      <th>Hillshade_Noon</th>\n",
              "      <th>Hillshade_3pm</th>\n",
              "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
              "      <th>Wilderness_Area1</th>\n",
              "      <th>Wilderness_Area2</th>\n",
              "      <th>Wilderness_Area3</th>\n",
              "      <th>Wilderness_Area4</th>\n",
              "      <th>Soil_Type1</th>\n",
              "      <th>Soil_Type2</th>\n",
              "      <th>Soil_Type3</th>\n",
              "      <th>Soil_Type4</th>\n",
              "      <th>Soil_Type5</th>\n",
              "      <th>Soil_Type6</th>\n",
              "      <th>Soil_Type7</th>\n",
              "      <th>Soil_Type8</th>\n",
              "      <th>Soil_Type9</th>\n",
              "      <th>Soil_Type10</th>\n",
              "      <th>Soil_Type11</th>\n",
              "      <th>Soil_Type12</th>\n",
              "      <th>Soil_Type13</th>\n",
              "      <th>Soil_Type14</th>\n",
              "      <th>Soil_Type15</th>\n",
              "      <th>Soil_Type16</th>\n",
              "      <th>Soil_Type17</th>\n",
              "      <th>Soil_Type18</th>\n",
              "      <th>Soil_Type19</th>\n",
              "      <th>Soil_Type20</th>\n",
              "      <th>Soil_Type21</th>\n",
              "      <th>Soil_Type22</th>\n",
              "      <th>Soil_Type23</th>\n",
              "      <th>Soil_Type24</th>\n",
              "      <th>Soil_Type25</th>\n",
              "      <th>Soil_Type26</th>\n",
              "      <th>Soil_Type27</th>\n",
              "      <th>Soil_Type28</th>\n",
              "      <th>Soil_Type29</th>\n",
              "      <th>Soil_Type30</th>\n",
              "      <th>Soil_Type31</th>\n",
              "      <th>Soil_Type32</th>\n",
              "      <th>Soil_Type33</th>\n",
              "      <th>Soil_Type34</th>\n",
              "      <th>Soil_Type35</th>\n",
              "      <th>Soil_Type36</th>\n",
              "      <th>Soil_Type37</th>\n",
              "      <th>Soil_Type38</th>\n",
              "      <th>Soil_Type39</th>\n",
              "      <th>Soil_Type40</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "      <td>581012.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2959.365301</td>\n",
              "      <td>155.656807</td>\n",
              "      <td>14.103704</td>\n",
              "      <td>269.428217</td>\n",
              "      <td>46.418855</td>\n",
              "      <td>2350.146611</td>\n",
              "      <td>212.146049</td>\n",
              "      <td>223.318716</td>\n",
              "      <td>142.528263</td>\n",
              "      <td>1980.291226</td>\n",
              "      <td>0.448865</td>\n",
              "      <td>0.051434</td>\n",
              "      <td>0.436074</td>\n",
              "      <td>0.063627</td>\n",
              "      <td>0.005217</td>\n",
              "      <td>0.012952</td>\n",
              "      <td>0.008301</td>\n",
              "      <td>0.021335</td>\n",
              "      <td>0.002749</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>0.001974</td>\n",
              "      <td>0.056168</td>\n",
              "      <td>0.021359</td>\n",
              "      <td>0.051584</td>\n",
              "      <td>0.030001</td>\n",
              "      <td>0.001031</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.004897</td>\n",
              "      <td>0.005890</td>\n",
              "      <td>0.003268</td>\n",
              "      <td>0.006921</td>\n",
              "      <td>0.015936</td>\n",
              "      <td>0.001442</td>\n",
              "      <td>0.057439</td>\n",
              "      <td>0.099399</td>\n",
              "      <td>0.036622</td>\n",
              "      <td>0.000816</td>\n",
              "      <td>0.004456</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.001628</td>\n",
              "      <td>0.198356</td>\n",
              "      <td>0.051927</td>\n",
              "      <td>0.044175</td>\n",
              "      <td>0.090392</td>\n",
              "      <td>0.077716</td>\n",
              "      <td>0.002773</td>\n",
              "      <td>0.003255</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.026803</td>\n",
              "      <td>0.023762</td>\n",
              "      <td>0.015060</td>\n",
              "      <td>2.051471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>279.984734</td>\n",
              "      <td>111.913721</td>\n",
              "      <td>7.488242</td>\n",
              "      <td>212.549356</td>\n",
              "      <td>58.295232</td>\n",
              "      <td>1559.254870</td>\n",
              "      <td>26.769889</td>\n",
              "      <td>19.768697</td>\n",
              "      <td>38.274529</td>\n",
              "      <td>1324.195210</td>\n",
              "      <td>0.497379</td>\n",
              "      <td>0.220882</td>\n",
              "      <td>0.495897</td>\n",
              "      <td>0.244087</td>\n",
              "      <td>0.072039</td>\n",
              "      <td>0.113066</td>\n",
              "      <td>0.090731</td>\n",
              "      <td>0.144499</td>\n",
              "      <td>0.052356</td>\n",
              "      <td>0.105775</td>\n",
              "      <td>0.013442</td>\n",
              "      <td>0.017550</td>\n",
              "      <td>0.044387</td>\n",
              "      <td>0.230245</td>\n",
              "      <td>0.144579</td>\n",
              "      <td>0.221186</td>\n",
              "      <td>0.170590</td>\n",
              "      <td>0.032092</td>\n",
              "      <td>0.002272</td>\n",
              "      <td>0.069804</td>\n",
              "      <td>0.076518</td>\n",
              "      <td>0.057077</td>\n",
              "      <td>0.082902</td>\n",
              "      <td>0.125228</td>\n",
              "      <td>0.037950</td>\n",
              "      <td>0.232681</td>\n",
              "      <td>0.299197</td>\n",
              "      <td>0.187833</td>\n",
              "      <td>0.028551</td>\n",
              "      <td>0.066605</td>\n",
              "      <td>0.043193</td>\n",
              "      <td>0.040318</td>\n",
              "      <td>0.398762</td>\n",
              "      <td>0.221879</td>\n",
              "      <td>0.205483</td>\n",
              "      <td>0.286743</td>\n",
              "      <td>0.267725</td>\n",
              "      <td>0.052584</td>\n",
              "      <td>0.056957</td>\n",
              "      <td>0.014310</td>\n",
              "      <td>0.022641</td>\n",
              "      <td>0.161508</td>\n",
              "      <td>0.152307</td>\n",
              "      <td>0.121791</td>\n",
              "      <td>1.396504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1859.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-173.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2809.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>108.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1106.000000</td>\n",
              "      <td>198.000000</td>\n",
              "      <td>213.000000</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>1024.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2996.000000</td>\n",
              "      <td>127.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>1997.000000</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>226.000000</td>\n",
              "      <td>143.000000</td>\n",
              "      <td>1710.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3163.000000</td>\n",
              "      <td>260.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>384.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>3328.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>237.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>2550.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3858.000000</td>\n",
              "      <td>360.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>1397.000000</td>\n",
              "      <td>601.000000</td>\n",
              "      <td>7117.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>7173.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ff4d8bb-03c2-4fd2-b8c1-b807c4a535b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ff4d8bb-03c2-4fd2-b8c1-b807c4a535b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ff4d8bb-03c2-4fd2-b8c1-b807c4a535b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           Elevation         Aspect  ...    Soil_Type40          class\n",
              "count  581012.000000  581012.000000  ...  581012.000000  581012.000000\n",
              "mean     2959.365301     155.656807  ...       0.015060       2.051471\n",
              "std       279.984734     111.913721  ...       0.121791       1.396504\n",
              "min      1859.000000       0.000000  ...       0.000000       1.000000\n",
              "25%      2809.000000      58.000000  ...       0.000000       1.000000\n",
              "50%      2996.000000     127.000000  ...       0.000000       2.000000\n",
              "75%      3163.000000     260.000000  ...       0.000000       2.000000\n",
              "max      3858.000000     360.000000  ...       1.000000       7.000000\n",
              "\n",
              "[8 rows x 55 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "# Get metadata on dataset\n",
        "cover_dataframe.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L-nwhJM0kkE"
      },
      "outputs": [],
      "source": [
        "for column in list(cover_dataframe):\n",
        "\n",
        "  cover_dataframe[column].fillna(0, inplace = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4ZzEK9-rKYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20ce0eb-b52a-410b-f126-dcce3966abad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         5\n",
            "1         5\n",
            "2         2\n",
            "3         2\n",
            "4         5\n",
            "         ..\n",
            "581007    3\n",
            "581008    3\n",
            "581009    3\n",
            "581010    3\n",
            "581011    3\n",
            "Name: class, Length: 581012, dtype: int64\n",
            "[5 2 1 7 3 6 4]\n",
            "        Elevation  Aspect  Slope  ...  Soil_Type38  Soil_Type39  Soil_Type40\n",
            "0            2596      51      3  ...            0            0            0\n",
            "1            2590      56      2  ...            0            0            0\n",
            "2            2804     139      9  ...            0            0            0\n",
            "3            2785     155     18  ...            0            0            0\n",
            "4            2595      45      2  ...            0            0            0\n",
            "...           ...     ...    ...  ...          ...          ...          ...\n",
            "581007       2396     153     20  ...            0            0            0\n",
            "581008       2391     152     19  ...            0            0            0\n",
            "581009       2386     159     17  ...            0            0            0\n",
            "581010       2384     170     15  ...            0            0            0\n",
            "581011       2383     165     13  ...            0            0            0\n",
            "\n",
            "[581012 rows x 54 columns]\n",
            "           Elevation         Aspect  ...    Soil_Type39    Soil_Type40\n",
            "count  581012.000000  581012.000000  ...  581012.000000  581012.000000\n",
            "mean     2959.365301     155.656807  ...       0.023762       0.015060\n",
            "std       279.984734     111.913721  ...       0.152307       0.121791\n",
            "min      1859.000000       0.000000  ...       0.000000       0.000000\n",
            "25%      2809.000000      58.000000  ...       0.000000       0.000000\n",
            "50%      2996.000000     127.000000  ...       0.000000       0.000000\n",
            "75%      3163.000000     260.000000  ...       0.000000       0.000000\n",
            "max      3858.000000     360.000000  ...       1.000000       1.000000\n",
            "\n",
            "[8 rows x 54 columns]\n"
          ]
        }
      ],
      "source": [
        "# Further preprocess data\n",
        "\n",
        "# Extract labels column\n",
        "\n",
        "labels = cover_dataframe['class']\n",
        "print(labels)\n",
        "print(labels.unique())\n",
        "\n",
        "features = cover_dataframe.drop('class', axis=1)\n",
        "\n",
        "print(features)\n",
        "print(features.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eBTOs2axZkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dde74d4-6890-42ca-a440-65bdde96be86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Elevation  Aspect  Slope  ...  Soil_Type38  Soil_Type39  Soil_Type40\n",
            "0            2596      51      3  ...            0            0            0\n",
            "1            2590      56      2  ...            0            0            0\n",
            "2            2804     139      9  ...            0            0            0\n",
            "3            2785     155     18  ...            0            0            0\n",
            "4            2595      45      2  ...            0            0            0\n",
            "...           ...     ...    ...  ...          ...          ...          ...\n",
            "581007       2396     153     20  ...            0            0            0\n",
            "581008       2391     152     19  ...            0            0            0\n",
            "581009       2386     159     17  ...            0            0            0\n",
            "581010       2384     170     15  ...            0            0            0\n",
            "581011       2383     165     13  ...            0            0            0\n",
            "\n",
            "[581012 rows x 54 columns]\n",
            "           Elevation         Aspect  ...    Soil_Type39    Soil_Type40\n",
            "count  581012.000000  581012.000000  ...  581012.000000  581012.000000\n",
            "mean     2959.365301     155.656807  ...       0.023762       0.015060\n",
            "std       279.984734     111.913721  ...       0.152307       0.121791\n",
            "min      1859.000000       0.000000  ...       0.000000       0.000000\n",
            "25%      2809.000000      58.000000  ...       0.000000       0.000000\n",
            "50%      2996.000000     127.000000  ...       0.000000       0.000000\n",
            "75%      3163.000000     260.000000  ...       0.000000       0.000000\n",
            "max      3858.000000     360.000000  ...       1.000000       1.000000\n",
            "\n",
            "[8 rows x 54 columns]\n"
          ]
        }
      ],
      "source": [
        "# convert the categorical features in features dataframe to one-hot encoding vectors \n",
        "\n",
        "features = pd.get_dummies(features)\n",
        "\n",
        "print(features)\n",
        "print(features.describe())\n",
        "\n",
        "# Split dataset into training and testing dataset\n",
        "\n",
        "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, labels, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT_RFPJn6HC4"
      },
      "outputs": [],
      "source": [
        "# Use sklearn's ColumnTransformer to scale features of dataset \n",
        "#print(features_train)\n",
        "features_headers = list(features_train)\n",
        "features_headers_to_scale = features_headers[:10]\n",
        "\n",
        "ct = ColumnTransformer(\n",
        "    [(\"numeric\", StandardScaler(), features_headers_to_scale)],\n",
        "    remainder='passthrough'\n",
        "    )\n",
        "og_features_train = features_train\n",
        "og_features_test = features_test\n",
        "features_train = ct.fit_transform(features_train)\n",
        "#print(features_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEZ5_mqgAhHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b61cd8-98a4-4372-f2bc-3605156b1e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 7 1 3 6 5 4]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# No need for labelEncoder since labels are already numeric\n",
        "# Use to_categorical to transform labels into binary vectors\n",
        "print(labels_train.unique())\n",
        "labels_train = tf.keras.utils.to_categorical(labels_train)\n",
        "labels_test = tf.keras.utils.to_categorical(labels_test)\n",
        "print(labels_test[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axyb76_Wf_hy"
      },
      "source": [
        "# Build and Train Model\n",
        "[ x ] Create simple tensorflow model\n",
        "\n",
        "[ x ] Decide parameters (layers, activation, epochs, etc.)\n",
        "\n",
        "[ x ] Confirm dimensions\n",
        "\n",
        "[ x ] Compile successfully\n",
        "\n",
        "[ x ] Train model with training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhKm_fWVNVJD"
      },
      "outputs": [],
      "source": [
        "# Create model object\n",
        "\n",
        "class Model:\n",
        "  def __init__(self, name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split):\n",
        "    self.name = name\n",
        "    self.model_arc = Sequential()\n",
        "    # Input layer\n",
        "    self.model_arc.add(InputLayer(input_shape= (train_inputs.shape[1], )))\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "      if layers[i] == 'dropout':\n",
        "        self.model_arc.add(Dropout(0.1))\n",
        "      else:\n",
        "        self.model_arc.add(Dense(layerSizes[i], activation = layers[i]))\n",
        "    \n",
        "    self.model_arc.compile(loss=loss,optimizer=opt,metrics=metrics)\n",
        "\n",
        "    stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "    self.history = self.model_arc.fit(features_train, labels_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, validation_split=0.20, callbacks=[stop])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OU4UZeaf3X0"
      },
      "source": [
        "# Adjust Hyperparameters\n",
        "\n",
        "[ x ] Write out all hyperparameters in single cell\n",
        "\n",
        "[ x ] Decide systematic way to test different hyperparameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOxZTrBeFnOD"
      },
      "source": [
        "# Test different model structures and plot results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-zhlE-D_9j1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc68bc4-6082-4f8d-95fd-1eef2f2995f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3719/3719 [==============================] - 31s 8ms/step - loss: 0.6672 - accuracy: 0.7129 - val_loss: 0.6395 - val_accuracy: 0.7232\n",
            "Epoch 2/100\n",
            "3719/3719 [==============================] - 23s 6ms/step - loss: 0.6376 - accuracy: 0.7208 - val_loss: 0.6356 - val_accuracy: 0.7210\n",
            "Epoch 3/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6360 - accuracy: 0.7212 - val_loss: 0.6379 - val_accuracy: 0.7188\n",
            "Epoch 4/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6356 - accuracy: 0.7218 - val_loss: 0.6345 - val_accuracy: 0.7207\n",
            "Epoch 5/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6352 - accuracy: 0.7211 - val_loss: 0.6336 - val_accuracy: 0.7240\n",
            "Epoch 6/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6356 - accuracy: 0.7220 - val_loss: 0.6330 - val_accuracy: 0.7210\n",
            "Epoch 7/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6354 - accuracy: 0.7215 - val_loss: 0.6353 - val_accuracy: 0.7210\n",
            "Epoch 8/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.6354 - accuracy: 0.7214 - val_loss: 0.6359 - val_accuracy: 0.7177\n",
            "Epoch 9/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6351 - accuracy: 0.7215 - val_loss: 0.6328 - val_accuracy: 0.7258\n",
            "Epoch 10/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6353 - accuracy: 0.7214 - val_loss: 0.6327 - val_accuracy: 0.7241\n",
            "Epoch 11/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6352 - accuracy: 0.7215 - val_loss: 0.6340 - val_accuracy: 0.7235\n",
            "Epoch 12/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6351 - accuracy: 0.7219 - val_loss: 0.6343 - val_accuracy: 0.7213\n",
            "Epoch 13/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6350 - accuracy: 0.7213 - val_loss: 0.6375 - val_accuracy: 0.7234\n",
            "Epoch 14/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6352 - accuracy: 0.7219 - val_loss: 0.6340 - val_accuracy: 0.7211\n",
            "Epoch 15/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.6351 - accuracy: 0.7221 - val_loss: 0.6326 - val_accuracy: 0.7250\n",
            "Epoch 16/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6350 - accuracy: 0.7219 - val_loss: 0.6320 - val_accuracy: 0.7270\n",
            "Epoch 17/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6350 - accuracy: 0.7218 - val_loss: 0.6326 - val_accuracy: 0.7237\n",
            "Epoch 18/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6350 - accuracy: 0.7217 - val_loss: 0.6322 - val_accuracy: 0.7270\n",
            "Epoch 19/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6352 - accuracy: 0.7219 - val_loss: 0.6326 - val_accuracy: 0.7279\n",
            "Epoch 20/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6349 - accuracy: 0.7217 - val_loss: 0.6321 - val_accuracy: 0.7215\n",
            "Epoch 21/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6348 - accuracy: 0.7220 - val_loss: 0.6355 - val_accuracy: 0.7198\n",
            "Epoch 22/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6350 - accuracy: 0.7220 - val_loss: 0.6333 - val_accuracy: 0.7247\n",
            "Epoch 23/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6351 - accuracy: 0.7215 - val_loss: 0.6318 - val_accuracy: 0.7247\n",
            "Epoch 24/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6350 - accuracy: 0.7216 - val_loss: 0.6334 - val_accuracy: 0.7241\n",
            "Epoch 25/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6351 - accuracy: 0.7219 - val_loss: 0.6343 - val_accuracy: 0.7212\n",
            "Epoch 26/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6349 - accuracy: 0.7222 - val_loss: 0.6373 - val_accuracy: 0.7213\n",
            "Epoch 27/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6351 - accuracy: 0.7219 - val_loss: 0.6338 - val_accuracy: 0.7233\n",
            "Epoch 28/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6349 - accuracy: 0.7217 - val_loss: 0.6349 - val_accuracy: 0.7233\n",
            "Epoch 29/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6350 - accuracy: 0.7216 - val_loss: 0.6335 - val_accuracy: 0.7192\n",
            "Epoch 30/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6350 - accuracy: 0.7217 - val_loss: 0.6373 - val_accuracy: 0.7227\n",
            "Epoch 31/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6350 - accuracy: 0.7218 - val_loss: 0.6341 - val_accuracy: 0.7255\n",
            "Epoch 32/100\n",
            "3719/3719 [==============================] - 15s 4ms/step - loss: 0.6348 - accuracy: 0.7215 - val_loss: 0.6392 - val_accuracy: 0.7220\n",
            "Epoch 33/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.6350 - accuracy: 0.7218 - val_loss: 0.6351 - val_accuracy: 0.7233\n",
            "Epoch 00033: early stopping\n",
            "Epoch 1/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.6122 - accuracy: 0.7436 - val_loss: 0.5416 - val_accuracy: 0.7754\n",
            "Epoch 2/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.5115 - accuracy: 0.7843 - val_loss: 0.4925 - val_accuracy: 0.7981\n",
            "Epoch 3/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.4693 - accuracy: 0.8016 - val_loss: 0.4514 - val_accuracy: 0.8091\n",
            "Epoch 4/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.4346 - accuracy: 0.8168 - val_loss: 0.4226 - val_accuracy: 0.8215\n",
            "Epoch 5/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.4132 - accuracy: 0.8265 - val_loss: 0.4073 - val_accuracy: 0.8305\n",
            "Epoch 6/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3990 - accuracy: 0.8331 - val_loss: 0.3963 - val_accuracy: 0.8352\n",
            "Epoch 7/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3850 - accuracy: 0.8399 - val_loss: 0.3774 - val_accuracy: 0.8456\n",
            "Epoch 8/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3750 - accuracy: 0.8462 - val_loss: 0.3790 - val_accuracy: 0.8461\n",
            "Epoch 9/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3663 - accuracy: 0.8500 - val_loss: 0.3617 - val_accuracy: 0.8522\n",
            "Epoch 10/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3598 - accuracy: 0.8538 - val_loss: 0.3553 - val_accuracy: 0.8562\n",
            "Epoch 11/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3532 - accuracy: 0.8556 - val_loss: 0.3568 - val_accuracy: 0.8554\n",
            "Epoch 12/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3485 - accuracy: 0.8585 - val_loss: 0.3600 - val_accuracy: 0.8530\n",
            "Epoch 13/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3428 - accuracy: 0.8599 - val_loss: 0.3480 - val_accuracy: 0.8590\n",
            "Epoch 14/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3413 - accuracy: 0.8609 - val_loss: 0.3474 - val_accuracy: 0.8576\n",
            "Epoch 15/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3392 - accuracy: 0.8620 - val_loss: 0.3394 - val_accuracy: 0.8631\n",
            "Epoch 16/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3370 - accuracy: 0.8636 - val_loss: 0.3452 - val_accuracy: 0.8593\n",
            "Epoch 17/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3323 - accuracy: 0.8644 - val_loss: 0.3469 - val_accuracy: 0.8598\n",
            "Epoch 18/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3325 - accuracy: 0.8650 - val_loss: 0.3541 - val_accuracy: 0.8551\n",
            "Epoch 19/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3319 - accuracy: 0.8653 - val_loss: 0.3366 - val_accuracy: 0.8631\n",
            "Epoch 20/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3285 - accuracy: 0.8657 - val_loss: 0.3338 - val_accuracy: 0.8639\n",
            "Epoch 21/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3271 - accuracy: 0.8668 - val_loss: 0.3383 - val_accuracy: 0.8614\n",
            "Epoch 22/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3257 - accuracy: 0.8688 - val_loss: 0.3365 - val_accuracy: 0.8648\n",
            "Epoch 23/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3265 - accuracy: 0.8676 - val_loss: 0.3352 - val_accuracy: 0.8641\n",
            "Epoch 24/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3243 - accuracy: 0.8679 - val_loss: 0.3319 - val_accuracy: 0.8675\n",
            "Epoch 25/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3258 - accuracy: 0.8677 - val_loss: 0.3355 - val_accuracy: 0.8665\n",
            "Epoch 26/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3211 - accuracy: 0.8699 - val_loss: 0.3309 - val_accuracy: 0.8673\n",
            "Epoch 27/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3238 - accuracy: 0.8688 - val_loss: 0.3237 - val_accuracy: 0.8697\n",
            "Epoch 28/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3222 - accuracy: 0.8695 - val_loss: 0.3338 - val_accuracy: 0.8650\n",
            "Epoch 29/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3229 - accuracy: 0.8697 - val_loss: 0.3442 - val_accuracy: 0.8602\n",
            "Epoch 30/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3257 - accuracy: 0.8672 - val_loss: 0.3346 - val_accuracy: 0.8662\n",
            "Epoch 31/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3251 - accuracy: 0.8674 - val_loss: 0.3255 - val_accuracy: 0.8676\n",
            "Epoch 32/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3224 - accuracy: 0.8692 - val_loss: 0.3290 - val_accuracy: 0.8665\n",
            "Epoch 33/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3253 - accuracy: 0.8683 - val_loss: 0.3308 - val_accuracy: 0.8658\n",
            "Epoch 34/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3237 - accuracy: 0.8685 - val_loss: 0.3423 - val_accuracy: 0.8606\n",
            "Epoch 35/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3239 - accuracy: 0.8691 - val_loss: 0.3429 - val_accuracy: 0.8602\n",
            "Epoch 36/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3251 - accuracy: 0.8685 - val_loss: 0.3305 - val_accuracy: 0.8658\n",
            "Epoch 37/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3250 - accuracy: 0.8686 - val_loss: 0.3346 - val_accuracy: 0.8654\n",
            "Epoch 00037: early stopping\n",
            "Epoch 1/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.5155 - accuracy: 0.7768 - val_loss: 0.4620 - val_accuracy: 0.7992\n",
            "Epoch 2/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.4458 - accuracy: 0.8096 - val_loss: 0.4410 - val_accuracy: 0.8131\n",
            "Epoch 3/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.4250 - accuracy: 0.8193 - val_loss: 0.4123 - val_accuracy: 0.8262\n",
            "Epoch 4/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.4141 - accuracy: 0.8246 - val_loss: 0.4263 - val_accuracy: 0.8179\n",
            "Epoch 5/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.4050 - accuracy: 0.8288 - val_loss: 0.3948 - val_accuracy: 0.8351\n",
            "Epoch 6/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3991 - accuracy: 0.8314 - val_loss: 0.4055 - val_accuracy: 0.8273\n",
            "Epoch 7/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3934 - accuracy: 0.8336 - val_loss: 0.3994 - val_accuracy: 0.8294\n",
            "Epoch 8/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3892 - accuracy: 0.8356 - val_loss: 0.3999 - val_accuracy: 0.8354\n",
            "Epoch 9/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3857 - accuracy: 0.8377 - val_loss: 0.3970 - val_accuracy: 0.8361\n",
            "Epoch 10/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3824 - accuracy: 0.8387 - val_loss: 0.3820 - val_accuracy: 0.8403\n",
            "Epoch 11/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3803 - accuracy: 0.8403 - val_loss: 0.3759 - val_accuracy: 0.8422\n",
            "Epoch 12/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3777 - accuracy: 0.8411 - val_loss: 0.3819 - val_accuracy: 0.8394\n",
            "Epoch 13/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3755 - accuracy: 0.8424 - val_loss: 0.3880 - val_accuracy: 0.8390\n",
            "Epoch 14/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3743 - accuracy: 0.8430 - val_loss: 0.3827 - val_accuracy: 0.8417\n",
            "Epoch 15/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3732 - accuracy: 0.8435 - val_loss: 0.3760 - val_accuracy: 0.8429\n",
            "Epoch 16/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3710 - accuracy: 0.8446 - val_loss: 0.3773 - val_accuracy: 0.8444\n",
            "Epoch 17/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3697 - accuracy: 0.8454 - val_loss: 0.3682 - val_accuracy: 0.8466\n",
            "Epoch 18/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3687 - accuracy: 0.8456 - val_loss: 0.3733 - val_accuracy: 0.8431\n",
            "Epoch 19/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3694 - accuracy: 0.8458 - val_loss: 0.3760 - val_accuracy: 0.8425\n",
            "Epoch 20/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3676 - accuracy: 0.8461 - val_loss: 0.3880 - val_accuracy: 0.8429\n",
            "Epoch 21/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3676 - accuracy: 0.8466 - val_loss: 0.3709 - val_accuracy: 0.8463\n",
            "Epoch 22/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3659 - accuracy: 0.8470 - val_loss: 0.3799 - val_accuracy: 0.8423\n",
            "Epoch 23/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3659 - accuracy: 0.8469 - val_loss: 0.3647 - val_accuracy: 0.8492\n",
            "Epoch 24/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3644 - accuracy: 0.8480 - val_loss: 0.3720 - val_accuracy: 0.8473\n",
            "Epoch 25/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3644 - accuracy: 0.8478 - val_loss: 0.3810 - val_accuracy: 0.8421\n",
            "Epoch 26/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3633 - accuracy: 0.8484 - val_loss: 0.3714 - val_accuracy: 0.8454\n",
            "Epoch 27/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3638 - accuracy: 0.8478 - val_loss: 0.3684 - val_accuracy: 0.8474\n",
            "Epoch 28/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3627 - accuracy: 0.8496 - val_loss: 0.3925 - val_accuracy: 0.8393\n",
            "Epoch 29/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3613 - accuracy: 0.8499 - val_loss: 0.3793 - val_accuracy: 0.8426\n",
            "Epoch 30/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3622 - accuracy: 0.8488 - val_loss: 0.3781 - val_accuracy: 0.8465\n",
            "Epoch 31/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3609 - accuracy: 0.8500 - val_loss: 0.3667 - val_accuracy: 0.8473\n",
            "Epoch 32/100\n",
            "3719/3719 [==============================] - 16s 4ms/step - loss: 0.3613 - accuracy: 0.8501 - val_loss: 0.3866 - val_accuracy: 0.8367\n",
            "Epoch 33/100\n",
            "3719/3719 [==============================] - 17s 4ms/step - loss: 0.3611 - accuracy: 0.8498 - val_loss: 0.3706 - val_accuracy: 0.8463\n",
            "Epoch 00033: early stopping\n",
            "Epoch 1/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.4941 - accuracy: 0.7887 - val_loss: 0.4139 - val_accuracy: 0.8270\n",
            "Epoch 2/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3996 - accuracy: 0.8320 - val_loss: 0.4062 - val_accuracy: 0.8259\n",
            "Epoch 3/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3713 - accuracy: 0.8443 - val_loss: 0.3611 - val_accuracy: 0.8501\n",
            "Epoch 4/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3539 - accuracy: 0.8523 - val_loss: 0.3532 - val_accuracy: 0.8526\n",
            "Epoch 5/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3412 - accuracy: 0.8583 - val_loss: 0.3471 - val_accuracy: 0.8574\n",
            "Epoch 6/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3333 - accuracy: 0.8621 - val_loss: 0.3367 - val_accuracy: 0.8616\n",
            "Epoch 7/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3254 - accuracy: 0.8653 - val_loss: 0.3424 - val_accuracy: 0.8564\n",
            "Epoch 8/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3189 - accuracy: 0.8676 - val_loss: 0.3244 - val_accuracy: 0.8684\n",
            "Epoch 9/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3136 - accuracy: 0.8704 - val_loss: 0.3172 - val_accuracy: 0.8693\n",
            "Epoch 10/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3091 - accuracy: 0.8729 - val_loss: 0.3154 - val_accuracy: 0.8715\n",
            "Epoch 11/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3062 - accuracy: 0.8741 - val_loss: 0.3148 - val_accuracy: 0.8729\n",
            "Epoch 12/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3022 - accuracy: 0.8759 - val_loss: 0.2977 - val_accuracy: 0.8773\n",
            "Epoch 13/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3006 - accuracy: 0.8767 - val_loss: 0.3102 - val_accuracy: 0.8740\n",
            "Epoch 14/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2971 - accuracy: 0.8784 - val_loss: 0.3317 - val_accuracy: 0.8675\n",
            "Epoch 15/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2962 - accuracy: 0.8794 - val_loss: 0.3150 - val_accuracy: 0.8718\n",
            "Epoch 16/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2938 - accuracy: 0.8799 - val_loss: 0.2993 - val_accuracy: 0.8801\n",
            "Epoch 17/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2905 - accuracy: 0.8814 - val_loss: 0.3088 - val_accuracy: 0.8770\n",
            "Epoch 18/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2894 - accuracy: 0.8809 - val_loss: 0.2931 - val_accuracy: 0.8816\n",
            "Epoch 19/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2864 - accuracy: 0.8831 - val_loss: 0.3005 - val_accuracy: 0.8816\n",
            "Epoch 20/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2874 - accuracy: 0.8826 - val_loss: 0.2982 - val_accuracy: 0.8799\n",
            "Epoch 21/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2848 - accuracy: 0.8840 - val_loss: 0.2898 - val_accuracy: 0.8844\n",
            "Epoch 22/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2821 - accuracy: 0.8846 - val_loss: 0.2986 - val_accuracy: 0.8839\n",
            "Epoch 23/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2837 - accuracy: 0.8849 - val_loss: 0.3000 - val_accuracy: 0.8799\n",
            "Epoch 24/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2822 - accuracy: 0.8855 - val_loss: 0.2950 - val_accuracy: 0.8828\n",
            "Epoch 25/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2828 - accuracy: 0.8849 - val_loss: 0.2846 - val_accuracy: 0.8853\n",
            "Epoch 26/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2799 - accuracy: 0.8863 - val_loss: 0.2880 - val_accuracy: 0.8845\n",
            "Epoch 27/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2797 - accuracy: 0.8870 - val_loss: 0.2916 - val_accuracy: 0.8843\n",
            "Epoch 28/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2777 - accuracy: 0.8868 - val_loss: 0.2946 - val_accuracy: 0.8830\n",
            "Epoch 29/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2774 - accuracy: 0.8875 - val_loss: 0.3030 - val_accuracy: 0.8808\n",
            "Epoch 30/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2750 - accuracy: 0.8881 - val_loss: 0.2962 - val_accuracy: 0.8834\n",
            "Epoch 31/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2756 - accuracy: 0.8884 - val_loss: 0.2900 - val_accuracy: 0.8847\n",
            "Epoch 32/100\n",
            "3719/3719 [==============================] - 24s 7ms/step - loss: 0.2751 - accuracy: 0.8884 - val_loss: 0.3027 - val_accuracy: 0.8803\n",
            "Epoch 33/100\n",
            "3719/3719 [==============================] - 33s 9ms/step - loss: 0.2758 - accuracy: 0.8891 - val_loss: 0.2954 - val_accuracy: 0.8828\n",
            "Epoch 34/100\n",
            "3719/3719 [==============================] - 31s 8ms/step - loss: 0.2729 - accuracy: 0.8900 - val_loss: 0.2905 - val_accuracy: 0.8858\n",
            "Epoch 35/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2718 - accuracy: 0.8899 - val_loss: 0.2817 - val_accuracy: 0.8886\n",
            "Epoch 36/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2701 - accuracy: 0.8907 - val_loss: 0.2836 - val_accuracy: 0.8901\n",
            "Epoch 37/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2725 - accuracy: 0.8900 - val_loss: 0.2942 - val_accuracy: 0.8850\n",
            "Epoch 38/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.2741 - accuracy: 0.8892 - val_loss: 0.2898 - val_accuracy: 0.8871\n",
            "Epoch 39/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2735 - accuracy: 0.8903 - val_loss: 0.2960 - val_accuracy: 0.8854\n",
            "Epoch 40/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.2704 - accuracy: 0.8908 - val_loss: 0.2961 - val_accuracy: 0.8838\n",
            "Epoch 41/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2689 - accuracy: 0.8916 - val_loss: 0.2842 - val_accuracy: 0.8914\n",
            "Epoch 42/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2702 - accuracy: 0.8913 - val_loss: 0.2996 - val_accuracy: 0.8801\n",
            "Epoch 43/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2678 - accuracy: 0.8920 - val_loss: 0.2888 - val_accuracy: 0.8900\n",
            "Epoch 44/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2682 - accuracy: 0.8925 - val_loss: 0.2807 - val_accuracy: 0.8913\n",
            "Epoch 45/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2688 - accuracy: 0.8925 - val_loss: 0.2912 - val_accuracy: 0.8923\n",
            "Epoch 46/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2653 - accuracy: 0.8934 - val_loss: 0.2893 - val_accuracy: 0.8904\n",
            "Epoch 47/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2691 - accuracy: 0.8919 - val_loss: 0.2879 - val_accuracy: 0.8895\n",
            "Epoch 48/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2691 - accuracy: 0.8923 - val_loss: 0.2873 - val_accuracy: 0.8913\n",
            "Epoch 49/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2706 - accuracy: 0.8924 - val_loss: 0.2950 - val_accuracy: 0.8861\n",
            "Epoch 50/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2689 - accuracy: 0.8929 - val_loss: 0.3325 - val_accuracy: 0.8757\n",
            "Epoch 51/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2692 - accuracy: 0.8924 - val_loss: 0.2773 - val_accuracy: 0.8939\n",
            "Epoch 52/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2683 - accuracy: 0.8932 - val_loss: 0.2939 - val_accuracy: 0.8874\n",
            "Epoch 53/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2660 - accuracy: 0.8931 - val_loss: 0.3065 - val_accuracy: 0.8817\n",
            "Epoch 54/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2642 - accuracy: 0.8941 - val_loss: 0.2970 - val_accuracy: 0.8840\n",
            "Epoch 55/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2702 - accuracy: 0.8925 - val_loss: 0.2977 - val_accuracy: 0.8894\n",
            "Epoch 56/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2646 - accuracy: 0.8941 - val_loss: 0.2818 - val_accuracy: 0.8920\n",
            "Epoch 57/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2650 - accuracy: 0.8938 - val_loss: 0.2971 - val_accuracy: 0.8904\n",
            "Epoch 58/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2658 - accuracy: 0.8934 - val_loss: 0.2907 - val_accuracy: 0.8911\n",
            "Epoch 59/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2639 - accuracy: 0.8949 - val_loss: 0.2898 - val_accuracy: 0.8903\n",
            "Epoch 60/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2636 - accuracy: 0.8944 - val_loss: 0.2844 - val_accuracy: 0.8948\n",
            "Epoch 61/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2687 - accuracy: 0.8940 - val_loss: 0.2987 - val_accuracy: 0.8828\n",
            "Epoch 00061: early stopping\n",
            "Epoch 1/100\n",
            "3719/3719 [==============================] - 20s 5ms/step - loss: 0.5542 - accuracy: 0.7700 - val_loss: 0.4853 - val_accuracy: 0.7970\n",
            "Epoch 2/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.4627 - accuracy: 0.8098 - val_loss: 0.4499 - val_accuracy: 0.8119\n",
            "Epoch 3/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.4342 - accuracy: 0.8232 - val_loss: 0.4162 - val_accuracy: 0.8304\n",
            "Epoch 4/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.4177 - accuracy: 0.8307 - val_loss: 0.4157 - val_accuracy: 0.8335\n",
            "Epoch 5/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.4062 - accuracy: 0.8353 - val_loss: 0.4116 - val_accuracy: 0.8348\n",
            "Epoch 6/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3978 - accuracy: 0.8382 - val_loss: 0.4018 - val_accuracy: 0.8371\n",
            "Epoch 7/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3896 - accuracy: 0.8426 - val_loss: 0.3940 - val_accuracy: 0.8402\n",
            "Epoch 8/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3833 - accuracy: 0.8466 - val_loss: 0.3889 - val_accuracy: 0.8475\n",
            "Epoch 9/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3768 - accuracy: 0.8494 - val_loss: 0.3774 - val_accuracy: 0.8495\n",
            "Epoch 10/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3741 - accuracy: 0.8507 - val_loss: 0.3870 - val_accuracy: 0.8465\n",
            "Epoch 11/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3720 - accuracy: 0.8518 - val_loss: 0.3945 - val_accuracy: 0.8449\n",
            "Epoch 12/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3646 - accuracy: 0.8556 - val_loss: 0.3702 - val_accuracy: 0.8538\n",
            "Epoch 13/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3648 - accuracy: 0.8557 - val_loss: 0.3733 - val_accuracy: 0.8519\n",
            "Epoch 14/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3610 - accuracy: 0.8574 - val_loss: 0.3625 - val_accuracy: 0.8560\n",
            "Epoch 15/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3572 - accuracy: 0.8591 - val_loss: 0.3546 - val_accuracy: 0.8614\n",
            "Epoch 16/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3548 - accuracy: 0.8600 - val_loss: 0.3753 - val_accuracy: 0.8619\n",
            "Epoch 17/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3510 - accuracy: 0.8620 - val_loss: 0.3553 - val_accuracy: 0.8577\n",
            "Epoch 18/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3509 - accuracy: 0.8621 - val_loss: 0.3468 - val_accuracy: 0.8630\n",
            "Epoch 19/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3472 - accuracy: 0.8633 - val_loss: 0.3467 - val_accuracy: 0.8626\n",
            "Epoch 20/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3445 - accuracy: 0.8643 - val_loss: 0.3587 - val_accuracy: 0.8589\n",
            "Epoch 21/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3442 - accuracy: 0.8649 - val_loss: 0.3546 - val_accuracy: 0.8619\n",
            "Epoch 22/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3421 - accuracy: 0.8660 - val_loss: 0.3463 - val_accuracy: 0.8626\n",
            "Epoch 23/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3407 - accuracy: 0.8666 - val_loss: 0.3463 - val_accuracy: 0.8639\n",
            "Epoch 24/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3395 - accuracy: 0.8681 - val_loss: 0.3532 - val_accuracy: 0.8623\n",
            "Epoch 25/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3507 - accuracy: 0.8649 - val_loss: 0.3316 - val_accuracy: 0.8698\n",
            "Epoch 26/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3378 - accuracy: 0.8680 - val_loss: 0.3362 - val_accuracy: 0.8680\n",
            "Epoch 27/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3388 - accuracy: 0.8680 - val_loss: 0.3903 - val_accuracy: 0.8519\n",
            "Epoch 28/100\n",
            "3719/3719 [==============================] - 23s 6ms/step - loss: 0.3461 - accuracy: 0.8677 - val_loss: 0.3455 - val_accuracy: 0.8656\n",
            "Epoch 29/100\n",
            "3719/3719 [==============================] - 25s 7ms/step - loss: 0.3366 - accuracy: 0.8697 - val_loss: 0.3322 - val_accuracy: 0.8694\n",
            "Epoch 30/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3341 - accuracy: 0.8706 - val_loss: 0.3266 - val_accuracy: 0.8711\n",
            "Epoch 31/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3358 - accuracy: 0.8685 - val_loss: 0.3383 - val_accuracy: 0.8678\n",
            "Epoch 32/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3331 - accuracy: 0.8702 - val_loss: 0.3366 - val_accuracy: 0.8685\n",
            "Epoch 33/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3275 - accuracy: 0.8722 - val_loss: 0.3431 - val_accuracy: 0.8682\n",
            "Epoch 34/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3311 - accuracy: 0.8721 - val_loss: 0.3284 - val_accuracy: 0.8728\n",
            "Epoch 35/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3264 - accuracy: 0.8726 - val_loss: 0.3472 - val_accuracy: 0.8631\n",
            "Epoch 36/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3311 - accuracy: 0.8714 - val_loss: 0.3338 - val_accuracy: 0.8735\n",
            "Epoch 37/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3310 - accuracy: 0.8721 - val_loss: 0.3367 - val_accuracy: 0.8703\n",
            "Epoch 38/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3264 - accuracy: 0.8732 - val_loss: 0.3237 - val_accuracy: 0.8737\n",
            "Epoch 39/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3291 - accuracy: 0.8728 - val_loss: 0.3401 - val_accuracy: 0.8679\n",
            "Epoch 40/100\n",
            "3719/3719 [==============================] - 20s 5ms/step - loss: 0.3285 - accuracy: 0.8722 - val_loss: 0.3282 - val_accuracy: 0.8720\n",
            "Epoch 41/100\n",
            "3719/3719 [==============================] - 20s 5ms/step - loss: 0.3242 - accuracy: 0.8746 - val_loss: 0.3326 - val_accuracy: 0.8690\n",
            "Epoch 42/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3205 - accuracy: 0.8756 - val_loss: 0.3218 - val_accuracy: 0.8752\n",
            "Epoch 43/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.3283 - accuracy: 0.8726 - val_loss: 0.3282 - val_accuracy: 0.8727\n",
            "Epoch 44/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3198 - accuracy: 0.8757 - val_loss: 0.3352 - val_accuracy: 0.8710\n",
            "Epoch 45/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3242 - accuracy: 0.8749 - val_loss: 0.3258 - val_accuracy: 0.8748\n",
            "Epoch 46/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3216 - accuracy: 0.8763 - val_loss: 0.3278 - val_accuracy: 0.8756\n",
            "Epoch 47/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3220 - accuracy: 0.8761 - val_loss: 0.3446 - val_accuracy: 0.8650\n",
            "Epoch 48/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3245 - accuracy: 0.8748 - val_loss: 0.3194 - val_accuracy: 0.8789\n",
            "Epoch 49/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3146 - accuracy: 0.8784 - val_loss: 0.3269 - val_accuracy: 0.8756\n",
            "Epoch 50/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3203 - accuracy: 0.8771 - val_loss: 0.3254 - val_accuracy: 0.8779\n",
            "Epoch 51/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3206 - accuracy: 0.8762 - val_loss: 0.3268 - val_accuracy: 0.8763\n",
            "Epoch 52/100\n",
            "3719/3719 [==============================] - 24s 6ms/step - loss: 0.3222 - accuracy: 0.8772 - val_loss: 0.3512 - val_accuracy: 0.8700\n",
            "Epoch 53/100\n",
            "3719/3719 [==============================] - 20s 5ms/step - loss: 0.3174 - accuracy: 0.8784 - val_loss: 0.3466 - val_accuracy: 0.8719\n",
            "Epoch 54/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3149 - accuracy: 0.8781 - val_loss: 0.3398 - val_accuracy: 0.8775\n",
            "Epoch 55/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3230 - accuracy: 0.8775 - val_loss: 0.3315 - val_accuracy: 0.8703\n",
            "Epoch 56/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3167 - accuracy: 0.8774 - val_loss: 0.3156 - val_accuracy: 0.8789\n",
            "Epoch 57/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3141 - accuracy: 0.8790 - val_loss: 0.3166 - val_accuracy: 0.8772\n",
            "Epoch 58/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3114 - accuracy: 0.8794 - val_loss: 0.3127 - val_accuracy: 0.8796\n",
            "Epoch 59/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3138 - accuracy: 0.8794 - val_loss: 0.3146 - val_accuracy: 0.8774\n",
            "Epoch 60/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3131 - accuracy: 0.8795 - val_loss: 0.3295 - val_accuracy: 0.8752\n",
            "Epoch 61/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3120 - accuracy: 0.8800 - val_loss: 0.3190 - val_accuracy: 0.8778\n",
            "Epoch 62/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3131 - accuracy: 0.8798 - val_loss: 0.3162 - val_accuracy: 0.8805\n",
            "Epoch 63/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3182 - accuracy: 0.8787 - val_loss: 0.3201 - val_accuracy: 0.8772\n",
            "Epoch 64/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3122 - accuracy: 0.8801 - val_loss: 0.3335 - val_accuracy: 0.8745\n",
            "Epoch 65/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3187 - accuracy: 0.8770 - val_loss: 0.3142 - val_accuracy: 0.8814\n",
            "Epoch 66/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3085 - accuracy: 0.8807 - val_loss: 0.3274 - val_accuracy: 0.8749\n",
            "Epoch 67/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3115 - accuracy: 0.8797 - val_loss: 0.3408 - val_accuracy: 0.8685\n",
            "Epoch 68/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.3143 - accuracy: 0.8785 - val_loss: 0.3183 - val_accuracy: 0.8799\n",
            "Epoch 00068: early stopping\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Basic module structure\n",
        "train_inputs = features_train\n",
        "train_labels = labels_train\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 100\n",
        "LR = 0.01\n",
        "opt  = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "patience = 10\n",
        "val_split = 0.20\n",
        "\n",
        "# Module structure 1\n",
        "name = 'model_1'\n",
        "layers = ['softmax']\n",
        "layerSizes = ['8']\n",
        "model_1 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model structure 2\n",
        "name = 'model_2'\n",
        "layers = ['relu', 'sigmoid','relu', 'softmax']\n",
        "layerSizes = ['128','128','128','8']\n",
        "model_2 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model structure 3\n",
        "name = 'model_3'\n",
        "layers = ['relu', 'softmax']\n",
        "layerSizes = ['128','8']\n",
        "model_3 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model structure 4\n",
        "name = 'model_4'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['128','128','8']\n",
        "model_4 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model structure 5\n",
        "name = 'model_5'\n",
        "layers = ['relu','relu', 'relu', 'softmax']\n",
        "layerSizes = ['128','128','128','8']\n",
        "model_5 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YitVeUiXLMCw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "8c93b5a6-84e1-47c3-9d2a-74ed9ec30947"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Create plot figure\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "# Plot model accuracy\n",
        "plt.title('Model Accuracy Different Structures')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(model_1.history.history['accuracy'])\n",
        "plt.plot(model_2.history.history['accuracy'])\n",
        "plt.plot(model_3.history.history['accuracy'])\n",
        "plt.plot(model_4.history.history['accuracy'])\n",
        "plt.plot(model_5.history.history['accuracy'])\n",
        "plt.legend(['model_1','model_2','model_3','model_4', 'model_5'], loc='lower right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_structures_acc.png')\n",
        "\n",
        "# Plot model loss\n",
        "plt.title('Model Loss Different Structures')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(model_1.history.history['loss'])\n",
        "plt.plot(model_2.history.history['loss'])\n",
        "plt.plot(model_3.history.history['loss'])\n",
        "plt.plot(model_4.history.history['loss'])\n",
        "plt.plot(model_5.history.history['loss'])\n",
        "plt.legend(['model_1','model_2','model_3','model_4', 'model_5'], loc='upper right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_structures_loss.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4wbagVfDlL"
      },
      "source": [
        "# From best model structure, test different layer sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7qro_kTEzFP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "a5f4dd34-ef7e-4ef2-f8e5-287b65bba416"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Basic module structure\\ntrain_inputs = features_train\\ntrain_labels = labels_train\\nloss = 'categorical_crossentropy'\\nmetrics = ['accuracy']\\nEPOCHS = 100\\nBATCH_SIZE = 100\\nLR = 0.01\\nopt  = tf.keras.optimizers.Adam(learning_rate=LR)\\npatience = 5\\nval_split = 0.20\\n\\n# Model layer size 1\\nname = 'model_4_1'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['8','8','8']\\nmodel_4_1 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 2\\nname = 'model_4_2'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['64','64','8']\\nmodel_4_2 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 3\\nname = 'model_4_3'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['128','128','8']\\nmodel_4_3 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 4\\nname = 'model_4_4'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['256','256','8']\\nmodel_4_4 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 5\\nname = 'model_4_5'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['128','256','8']\\nmodel_4_5 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "'''\n",
        "# Basic module structure\n",
        "train_inputs = features_train\n",
        "train_labels = labels_train\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 100\n",
        "LR = 0.01\n",
        "opt  = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "patience = 5\n",
        "val_split = 0.20\n",
        "\n",
        "# Model layer size 1\n",
        "name = 'model_4_1'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['8','8','8']\n",
        "model_4_1 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 2\n",
        "name = 'model_4_2'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['64','64','8']\n",
        "model_4_2 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 3\n",
        "name = 'model_4_3'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['128','128','8']\n",
        "model_4_3 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 4\n",
        "name = 'model_4_4'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['256','256','8']\n",
        "model_4_4 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 5\n",
        "name = 'model_4_5'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['128','256','8']\n",
        "model_4_5 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNRbYOINLylK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "8ceb3f52-2789-46e1-b218-c6ad0dee2788"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Create plot figure\\nfig = plt.figure()\\nax1 = fig.add_subplot(1, 1, 1)\\nplt.title('Model Accuracy Different Layer Sizes')\\nplt.ylabel('accuracy')\\nplt.xlabel('epoch')\\n\\n# Plot model accuracy\\nplt.plot(model_4_1.history.history['accuracy'])\\nplt.plot(model_4_2.history.history['accuracy'])\\nplt.plot(model_4_3.history.history['accuracy'])\\nplt.plot(model_4_4.history.history['accuracy'])\\nplt.plot(model_4_5.history.history['accuracy'])\\nplt.legend(['model_4_1','model_4_2','model_4_3','model_4_4','model_4_5'], loc='lower_right')\\nplt.show()\\nplt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_acc.png')\\n\\n# Plot model loss\\nplt.title('Model Loss Different Layer Sizes')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.plot(model_4_1.history.history['loss'])\\nplt.plot(model_4_2.history.history['loss'])\\nplt.plot(model_4_3.history.history['loss'])\\nplt.plot(model_4_4.history.history['loss'])\\nplt.plot(model_4_5.history.history['loss'])\\nplt.legend(['model_4_1','model_4_2','model_4_3','model_4_4','model_4_5'], loc='upper right')\\nplt.show()\\nplt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_loss.png')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "'''\n",
        "# Create plot figure\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "plt.title('Model Accuracy Different Layer Sizes')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "# Plot model accuracy\n",
        "plt.plot(model_4_1.history.history['accuracy'])\n",
        "plt.plot(model_4_2.history.history['accuracy'])\n",
        "plt.plot(model_4_3.history.history['accuracy'])\n",
        "plt.plot(model_4_4.history.history['accuracy'])\n",
        "plt.plot(model_4_5.history.history['accuracy'])\n",
        "plt.legend(['model_4_1','model_4_2','model_4_3','model_4_4','model_4_5'], loc='lower_right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_acc.png')\n",
        "\n",
        "# Plot model loss\n",
        "plt.title('Model Loss Different Layer Sizes')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(model_4_1.history.history['loss'])\n",
        "plt.plot(model_4_2.history.history['loss'])\n",
        "plt.plot(model_4_3.history.history['loss'])\n",
        "plt.plot(model_4_4.history.history['loss'])\n",
        "plt.plot(model_4_5.history.history['loss'])\n",
        "plt.legend(['model_4_1','model_4_2','model_4_3','model_4_4','model_4_5'], loc='upper right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_loss.png')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Basic module structure\n",
        "train_inputs = features_train\n",
        "train_labels = labels_train\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 100\n",
        "LR = 0.01\n",
        "opt  = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "patience = 5\n",
        "val_split = 0.20\n",
        "\n",
        "# Model layer size 4\n",
        "name = 'model_4_4'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['256','256','8']\n",
        "model_4_4 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 5\n",
        "name = 'model_4_5'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['512','512','8']\n",
        "model_4_5 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 6\n",
        "name = 'model_4_6'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['1024','1024','8']\n",
        "model_4_6 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "gexhqXwnNoZN",
        "outputId": "656387f4-1718-4c3d-bfc7-4ad70436cfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Basic module structure\\ntrain_inputs = features_train\\ntrain_labels = labels_train\\nloss = 'categorical_crossentropy'\\nmetrics = ['accuracy']\\nEPOCHS = 100\\nBATCH_SIZE = 100\\nLR = 0.01\\nopt  = tf.keras.optimizers.Adam(learning_rate=LR)\\npatience = 5\\nval_split = 0.20\\n\\n# Model layer size 4\\nname = 'model_4_4'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['256','256','8']\\nmodel_4_4 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 5\\nname = 'model_4_5'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['512','512','8']\\nmodel_4_5 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 6\\nname = 'model_4_6'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['1024','1024','8']\\nmodel_4_6 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Create plot figure\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "plt.title('Model Accuracy Different Layer Sizes Cont.')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "# Plot model accuracy\n",
        "plt.plot(model_4_4.history.history['accuracy'])\n",
        "plt.plot(model_4_5.history.history['accuracy'])\n",
        "plt.plot(model_4_6.history.history['accuracy'])\n",
        "plt.legend(['model_4_4','model_4_5','model_4_6'], loc='lower right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_acc_cont.png')\n",
        "\n",
        "# Plot model loss\n",
        "plt.title('Model Loss Different Layer Sizes Cont.')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(model_4_4.history.history['loss'])\n",
        "plt.plot(model_4_5.history.history['loss'])\n",
        "plt.plot(model_4_6.history.history['loss'])\n",
        "plt.legend(['model_4_4','model_4_5','model_4_6'], loc='upper right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_loss_cont.png')\n",
        "'''"
      ],
      "metadata": {
        "id": "E8q8U0RWN5aD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "0ab0ec88-c084-498c-e53d-0a5a7f0886e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Create plot figure\\nfig = plt.figure()\\nax1 = fig.add_subplot(1, 1, 1)\\nplt.title('Model Accuracy Different Layer Sizes Cont.')\\nplt.ylabel('accuracy')\\nplt.xlabel('epoch')\\n\\n# Plot model accuracy\\nplt.plot(model_4_4.history.history['accuracy'])\\nplt.plot(model_4_5.history.history['accuracy'])\\nplt.plot(model_4_6.history.history['accuracy'])\\nplt.legend(['model_4_4','model_4_5','model_4_6'], loc='lower right')\\nplt.show()\\nplt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_acc_cont.png')\\n\\n# Plot model loss\\nplt.title('Model Loss Different Layer Sizes Cont.')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.plot(model_4_4.history.history['loss'])\\nplt.plot(model_4_5.history.history['loss'])\\nplt.plot(model_4_6.history.history['loss'])\\nplt.legend(['model_4_4','model_4_5','model_4_6'], loc='upper right')\\nplt.show()\\nplt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_sizes_loss_cont.png')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying Dropout to see changes"
      ],
      "metadata": {
        "id": "FgURR3RenoxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Basic module structure\n",
        "train_inputs = features_train\n",
        "train_labels = labels_train\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 100\n",
        "LR = 0.01\n",
        "opt  = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "patience = 5\n",
        "val_split = 0.20\n",
        "\n",
        "# Model layer size 4 with 1 dropout layer\n",
        "name = 'model_4_4_2'\n",
        "layers = ['relu','dropout','relu','softmax']\n",
        "layerSizes = ['256','','256','8']\n",
        "model_4_4_2 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 4 with 1 dropout layer\n",
        "name = 'model_4_4_3'\n",
        "layers = ['relu','dropout','relu','dropout', 'softmax']\n",
        "layerSizes = ['256','','256','','8']\n",
        "model_4_4_3 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 4\n",
        "name = 'model_4_4_1'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['256','256','8']\n",
        "model_4_4_1 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "QWFqmdNBnnod",
        "outputId": "e8c9ad50-1028-430b-ad72-f95c2ece7685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Basic module structure\\ntrain_inputs = features_train\\ntrain_labels = labels_train\\nloss = 'categorical_crossentropy'\\nmetrics = ['accuracy']\\nEPOCHS = 100\\nBATCH_SIZE = 100\\nLR = 0.01\\nopt  = tf.keras.optimizers.Adam(learning_rate=LR)\\npatience = 5\\nval_split = 0.20\\n\\n# Model layer size 4 with 1 dropout layer\\nname = 'model_4_4_2'\\nlayers = ['relu','dropout','relu','softmax']\\nlayerSizes = ['256','','256','8']\\nmodel_4_4_2 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 4 with 1 dropout layer\\nname = 'model_4_4_3'\\nlayers = ['relu','dropout','relu','dropout', 'softmax']\\nlayerSizes = ['256','','256','','8']\\nmodel_4_4_3 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 4\\nname = 'model_4_4_1'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['256','256','8']\\nmodel_4_4_1 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Create plot figure\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "plt.title('Model Accuracy w/wo dropout')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "# Plot model accuracy\n",
        "plt.plot(model_4_4_1.history.history['accuracy'])\n",
        "plt.plot(model_4_4_2.history.history['accuracy'])\n",
        "plt.plot(model_4_4_3.history.history['accuracy'])\n",
        "plt.legend(['model_4_4_1','model_4_4_2','model_4_4_3'], loc='lower right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/dropout_acc.png')\n",
        "\n",
        "# Create plot figure\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "plt.title('Model Loss w/wo dropout')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "# Plot model loss\n",
        "plt.plot(model_4_4_1.history.history['loss'])\n",
        "plt.plot(model_4_4_2.history.history['loss'])\n",
        "plt.plot(model_4_4_3.history.history['loss'])\n",
        "plt.legend(['model_4_4_1','model_4_4_2','model_4_4_3'], loc='upper right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/dropout_loss.png')\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "yJTHE1RLoFds",
        "outputId": "91f430e0-48ee-4c71-82cf-d3b5a697e250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Create plot figure\\nfig = plt.figure()\\nax1 = fig.add_subplot(1, 1, 1)\\nplt.title('Model Accuracy w/wo dropout')\\nplt.ylabel('accuracy')\\nplt.xlabel('epoch')\\n\\n# Plot model accuracy\\nplt.plot(model_4_4_1.history.history['accuracy'])\\nplt.plot(model_4_4_2.history.history['accuracy'])\\nplt.plot(model_4_4_3.history.history['accuracy'])\\nplt.legend(['model_4_4_1','model_4_4_2','model_4_4_3'], loc='lower right')\\nplt.show()\\nplt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/dropout_acc.png')\\n\\n# Create plot figure\\nfig = plt.figure()\\nax1 = fig.add_subplot(1, 1, 1)\\nplt.title('Model Loss w/wo dropout')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\n\\n# Plot model loss\\nplt.plot(model_4_4_1.history.history['loss'])\\nplt.plot(model_4_4_2.history.history['loss'])\\nplt.plot(model_4_4_3.history.history['loss'])\\nplt.legend(['model_4_4_1','model_4_4_2','model_4_4_3'], loc='upper right')\\nplt.show()\\nplt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/dropout_loss.png')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try different batch sizes"
      ],
      "metadata": {
        "id": "tS1nA9B6uYf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Basic module structure\n",
        "train_inputs = features_train\n",
        "train_labels = labels_train\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 100\n",
        "LR = 0.01\n",
        "opt  = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "patience = 5\n",
        "val_split = 0.20\n",
        "\n",
        "# Model layer size 4 with no dropout layer and batch size 100\n",
        "name = 'model_4_4_1_1'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['256','256','8']\n",
        "model_4_4_1_1 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "'''\n",
        "# Model layer size 4 with no dropout layer and batch size 16\n",
        "BATCH_SIZE = 16\n",
        "name = 'model_4_4_1_2'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['256','256','8']\n",
        "model_4_4_1_2 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "\n",
        "# Model layer size 4 with no dropout layer and batch size 32\n",
        "BATCH_SIZE = 32\n",
        "name = 'model_4_4_1_3'\n",
        "layers = ['relu','relu','softmax']\n",
        "layerSizes = ['256','256','8']\n",
        "model_4_4_1_3 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3itWT7GduXma",
        "outputId": "eb55b57e-83ba-4e58-a639-c93c2d1531c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3719/3719 [==============================] - 30s 8ms/step - loss: 0.4944 - accuracy: 0.7891 - val_loss: 0.4226 - val_accuracy: 0.8230\n",
            "Epoch 2/100\n",
            "3719/3719 [==============================] - 28s 8ms/step - loss: 0.3849 - accuracy: 0.8400 - val_loss: 0.3552 - val_accuracy: 0.8530\n",
            "Epoch 3/100\n",
            "3719/3719 [==============================] - 32s 9ms/step - loss: 0.3453 - accuracy: 0.8581 - val_loss: 0.3463 - val_accuracy: 0.8569\n",
            "Epoch 4/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3243 - accuracy: 0.8679 - val_loss: 0.3167 - val_accuracy: 0.8704\n",
            "Epoch 5/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.3096 - accuracy: 0.8748 - val_loss: 0.2954 - val_accuracy: 0.8794\n",
            "Epoch 6/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2994 - accuracy: 0.8791 - val_loss: 0.3084 - val_accuracy: 0.8764\n",
            "Epoch 7/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2917 - accuracy: 0.8826 - val_loss: 0.2992 - val_accuracy: 0.8810\n",
            "Epoch 8/100\n",
            "3719/3719 [==============================] - 19s 5ms/step - loss: 0.2857 - accuracy: 0.8854 - val_loss: 0.2793 - val_accuracy: 0.8887\n",
            "Epoch 9/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2792 - accuracy: 0.8887 - val_loss: 0.2748 - val_accuracy: 0.8889\n",
            "Epoch 10/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2750 - accuracy: 0.8902 - val_loss: 0.2746 - val_accuracy: 0.8910\n",
            "Epoch 11/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2704 - accuracy: 0.8926 - val_loss: 0.2783 - val_accuracy: 0.8902\n",
            "Epoch 12/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2683 - accuracy: 0.8935 - val_loss: 0.2723 - val_accuracy: 0.8927\n",
            "Epoch 13/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2656 - accuracy: 0.8948 - val_loss: 0.2714 - val_accuracy: 0.8911\n",
            "Epoch 14/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2630 - accuracy: 0.8960 - val_loss: 0.2767 - val_accuracy: 0.8927\n",
            "Epoch 15/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2624 - accuracy: 0.8965 - val_loss: 0.2737 - val_accuracy: 0.8955\n",
            "Epoch 16/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2584 - accuracy: 0.8983 - val_loss: 0.2709 - val_accuracy: 0.8948\n",
            "Epoch 17/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2600 - accuracy: 0.8977 - val_loss: 0.2921 - val_accuracy: 0.8966\n",
            "Epoch 18/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2602 - accuracy: 0.8983 - val_loss: 0.2565 - val_accuracy: 0.9008\n",
            "Epoch 19/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2536 - accuracy: 0.9002 - val_loss: 0.2653 - val_accuracy: 0.8978\n",
            "Epoch 20/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2531 - accuracy: 0.9006 - val_loss: 0.2740 - val_accuracy: 0.8962\n",
            "Epoch 21/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2525 - accuracy: 0.9010 - val_loss: 0.2566 - val_accuracy: 0.8999\n",
            "Epoch 22/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2498 - accuracy: 0.9021 - val_loss: 0.2609 - val_accuracy: 0.8994\n",
            "Epoch 23/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2510 - accuracy: 0.9022 - val_loss: 0.2622 - val_accuracy: 0.8985\n",
            "Epoch 24/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2498 - accuracy: 0.9025 - val_loss: 0.2713 - val_accuracy: 0.8977\n",
            "Epoch 25/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2493 - accuracy: 0.9025 - val_loss: 0.2435 - val_accuracy: 0.9037\n",
            "Epoch 26/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2473 - accuracy: 0.9033 - val_loss: 0.2613 - val_accuracy: 0.9007\n",
            "Epoch 27/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2473 - accuracy: 0.9033 - val_loss: 0.2645 - val_accuracy: 0.8969\n",
            "Epoch 28/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2470 - accuracy: 0.9030 - val_loss: 0.2613 - val_accuracy: 0.8991\n",
            "Epoch 29/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2438 - accuracy: 0.9047 - val_loss: 0.2826 - val_accuracy: 0.8935\n",
            "Epoch 30/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2497 - accuracy: 0.9031 - val_loss: 0.2701 - val_accuracy: 0.8977\n",
            "Epoch 31/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2453 - accuracy: 0.9047 - val_loss: 0.2638 - val_accuracy: 0.9017\n",
            "Epoch 32/100\n",
            "3719/3719 [==============================] - 18s 5ms/step - loss: 0.2460 - accuracy: 0.9050 - val_loss: 0.2689 - val_accuracy: 0.8979\n",
            "Epoch 33/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2435 - accuracy: 0.9048 - val_loss: 0.2536 - val_accuracy: 0.9047\n",
            "Epoch 34/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2420 - accuracy: 0.9061 - val_loss: 0.2652 - val_accuracy: 0.9010\n",
            "Epoch 35/100\n",
            "3719/3719 [==============================] - 17s 5ms/step - loss: 0.2448 - accuracy: 0.9054 - val_loss: 0.2542 - val_accuracy: 0.9033\n",
            "Epoch 00035: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Model layer size 4 with no dropout layer and batch size 16\\nBATCH_SIZE = 16\\nname = 'model_4_4_1_2'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['256','256','8']\\nmodel_4_4_1_2 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\\n# Model layer size 4 with no dropout layer and batch size 32\\nBATCH_SIZE = 32\\nname = 'model_4_4_1_3'\\nlayers = ['relu','relu','softmax']\\nlayerSizes = ['256','256','8']\\nmodel_4_4_1_3 = Model(name, layers, layerSizes, train_inputs, train_labels, loss, opt, metrics, EPOCHS, BATCH_SIZE, patience, val_split)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create plot figure\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "plt.title('Model_4_4_1_1 Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "# Plot model accuracy\n",
        "plt.plot(model_4_4_1_1.history.history['accuracy'])\n",
        "plt.legend(['model_4_4_1_1'], loc='lower right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_batch_acc.png')\n",
        "\n",
        "# Create plot figure\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(1, 1, 1)\n",
        "plt.title('Model_4_4_1_1 Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "# Plot model loss\n",
        "plt.plot(model_4_4_1_1.history.history['loss'])\n",
        "plt.plot(model_4_4_1_2.history.history['loss'])\n",
        "plt.plot(model_4_4_1_3.history.history['loss'])\n",
        "plt.legend(['model_4_4_1_1','model_4_4_1_2','model_4_4_1_3'], loc='upper right')\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/Codecademy Projects/Tensorflow/diff_batch_loss.png')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "X1OPq6TZudIY",
        "outputId": "4dc2d74a-da1f-4ddd-97b4-d23a06052c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-93-0bb9b29f83f4>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ls16vgFgCuW"
      },
      "source": [
        "# Test Model\n",
        "\n",
        "[ x ] Set up testing metrics/baseline\n",
        "\n",
        "[ x ] Compile testing report on best model\n",
        "\n",
        "[ x ] Test model on testing set after training and validation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK_W1Gcjc-vQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba21124-3e47-4420-b4f7-e097709ac059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random pred. = 0.7775 %\n",
            "Pred. always type 1 = 36.4539 %\n",
            "Pred. always type 2 = 48.7194 %\n",
            "Pred. always type 3 = 6.1722 %\n",
            "Pred. always type 4 = 0.4733 %\n",
            "Pred. always type 5 = 1.6381 %\n",
            "Pred. always type 6 = 3.007 %\n",
            "Pred. always type 7 = 3.5361 %\n",
            "1.0\n",
            "Pred. always type 1 = 36.487 %\n",
            "Pred. always type 2 = 48.9221 %\n",
            "Pred. always type 3 = 6.0799 %\n",
            "Pred. always type 4 = 0.4707 %\n",
            "Pred. always type 5 = 1.617 %\n",
            "Pred. always type 6 = 2.9173 %\n",
            "Pred. always type 7 = 3.5059 %\n",
            "0.9999999999999999\n"
          ]
        }
      ],
      "source": [
        "# Use dummy classifier as baseline for comparison\n",
        "\n",
        "# Accuracy of making random prediction\n",
        "rand_dummy_classifier = DummyClassifier(strategy='uniform')\n",
        "\n",
        "rand_dummy_classifier.fit(og_features_train, labels_train)\n",
        "rand_dummy_classifier.predict(og_features_test)\n",
        "prob = rand_dummy_classifier.score(og_features_train, labels_train)\n",
        "print(\"Random pred. = \" + str(round(prob*100,4)) + ' %')\n",
        "\n",
        "# Accuracy of predicting single category every time for all categories\n",
        "total_prob = 0\n",
        "for i in range(1,8):\n",
        "  cat = np.zeros(8)\n",
        "  cat[i] = 1\n",
        "  const_dummy_classifier = DummyClassifier(strategy='constant', constant=cat)\n",
        "  const_dummy_classifier.fit(og_features_train, labels_train)\n",
        "  const_dummy_classifier.predict(og_features_test)\n",
        "  prob = const_dummy_classifier.score(og_features_train, labels_train)\n",
        "  total_prob += prob\n",
        "  print(\"Pred. always type \" + str(i) + \" = \" + str(round(prob*100,4)) + ' %')\n",
        "\n",
        "print(total_prob)\n",
        "\n",
        "\n",
        "total_prob = 0\n",
        "for i in range(1,8):\n",
        "  cat = np.zeros(8)\n",
        "  cat[i] = 1\n",
        "  const_dummy_classifier = DummyClassifier(strategy='constant', constant=cat)\n",
        "  const_dummy_classifier.fit(features_test, labels_test)\n",
        "  const_dummy_classifier.predict(features_test)\n",
        "  prob = const_dummy_classifier.score(features_test, labels_test)\n",
        "  total_prob += prob\n",
        "  print(\"Pred. always type \" + str(i) + \" = \" + str(round(prob*100,4)) + ' %')\n",
        "\n",
        "print(total_prob)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best model from experiments: model_4_4_1_1\n",
        "\n",
        "\n",
        "#tf.keras.models.save_model(model_4_4_1_1.model_arc,'/content/drive/MyDrive/Codecademy Projects/Tensorflow/')\n",
        "#chosen_model = tf.keras.models.load_model('/content/drive/MyDrive/Codecademy Projects/Tensorflow/')\n",
        "chosen_model = model_4_4_1_1.model_arc"
      ],
      "metadata": {
        "id": "-AaIB25IEtoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAm8fJ1fhbmT"
      },
      "source": [
        "# Deploy Model\n",
        "\n",
        "[ x ] Get model predictions of testing set\n",
        "\n",
        "[ x ] Save results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels_estimate = chosen_model.predict(og_features_test)\n",
        "print(labels_estimate)\n",
        "\n",
        "labels_estimate_typelist = np.argmax(labels_estimate, axis=1)\n",
        "print(labels_estimate_typelist)\n",
        "print(np.unique(labels_estimate_typelist))\n",
        "print(labels_test)\n",
        "labels_test_typelist = np.argmax(labels_test, axis=1)\n",
        "print(labels_test_typelist)\n",
        "print(np.unique(labels_test_typelist))\n",
        "\n",
        "print(classification_report(labels_test_typelist, labels_estimate_typelist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pcTB1xkODlg",
        "outputId": "205aac81-b28d-42f1-fb80-b364b585c7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "[1 1 1 ... 1 1 1]\n",
            "[1 2]\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n",
            "[2 1 1 ... 2 1 2]\n",
            "[1 2 3 4 5 6 7]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.37      0.96      0.54     42608\n",
            "           2       0.74      0.07      0.14     56497\n",
            "           3       0.00      0.00      0.00      7132\n",
            "           4       0.00      0.00      0.00       545\n",
            "           5       0.00      0.00      0.00      1864\n",
            "           6       0.00      0.00      0.00      3473\n",
            "           7       0.00      0.00      0.00      4084\n",
            "\n",
            "    accuracy                           0.39    116203\n",
            "   macro avg       0.16      0.15      0.10    116203\n",
            "weighted avg       0.50      0.39      0.26    116203\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxrYsTof44j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3003e25-5b76-4938-9e83-e4c57b59f8ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[41107  1501     0     0     0     0     0]\n",
            " [52264  4233     0     0     0     0     0]\n",
            " [ 7132     0     0     0     0     0     0]\n",
            " [  545     0     0     0     0     0     0]\n",
            " [ 1863     1     0     0     0     0     0]\n",
            " [ 3473     0     0     0     0     0     0]\n",
            " [ 4084     0     0     0     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(labels_test_typelist, labels_estimate_typelist))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ePI-2aVCXrDm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Codecademy Tensorflow Project",
      "provenance": [],
      "mount_file_id": "1JsdEGEsr2DvcwP2SLF_PtIxBX9duXITz",
      "authorship_tag": "ABX9TyMa2sQcbr+W+IEVzxf13ube",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}